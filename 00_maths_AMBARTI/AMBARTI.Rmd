---
title: "Additive Main Effect Bayesian Additive Regression Tree interaction models (AMBARTI)"
author:
- Estev√£o Batista do Prado
date: "`r format(Sys.time(), '%d %B %Y')`"
geometry: margin=1.5cm
tags: [nothing, nothingness]
output:
  pdf_document:
    fig_caption: true
    number_sections: true
    citation_package: natbib
biblio-style: "apalike"
link-citations: yes
bibliography: bibliography.bib
header-includes:
  - \usepackage{amsmath,amsfonts,amssymb}
  - \usepackage[ruled,longend]{algorithm2e}
  - \usepackage{mathtools}
  - \usepackage{bbm}
  - \SetKw{KwBy}{by}
  - \usepackage{natbib}
  - \usepackage{float}
  - \usepackage{geometry}
  - \usepackage{xcolor}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# AMBARTI
$$
\begin{aligned}
Y_{ij}|\textbf{x}_{ij}, \mathcal{T}, \mathcal{M}, \Theta, \sigma^{2} &\sim \mbox{N} \left( g_{i} + e_{j} + \sum_{t=1}^{T} h(\textbf{x}_{ij}, \mathcal{M}_{t}, \mathcal{T}_{t}), \sigma^{2} \right), \\
\end{aligned}
$$
where $y_{ij}$ is the yield for genotype $i$ and environment $j$, and $g_i$ and $e_j$ are the genotype and environment effects, respectively. $h$ is then the individual BART trees with $h(x_{ij}, \mathcal{M}_{t}, \mathcal{T}_{t}) = \mu_{t \ell}$. $g$ and $e$ are random effects, so there are variance terms $\sigma^{2}_g$, $\sigma^{2}_e$, and residual standard deviation $\sigma^{2}$, which all need to be estimated, and $\Theta = (\mathcal{G}, \mathcal{E}, \sigma^{2}_{g}, \sigma^{2}_{e})^\top$, with $\mathcal{G} = (g_{1}, \cdots, g_{I})$ and $\mathcal{E} = (e_{1}, \cdots, e_{J})$. In addition, consider the prior distribution for $\mu_{t\ell}$, $g_{i}$, $e_{j}$, $\sigma^{2}_g$, $\sigma^{2}_e$, and $\sigma^{2}$ as
$$
  \begin{aligned}
\mu_{t\ell}|\mathcal{T}_{t}  & \stackrel{\mathclap{\tiny \mbox{iid}}}{\sim} \mbox{N}(\mu_{\mu} = 0, \sigma^{2}_{\mu}),\\
g_{i}|\mathcal{T}_{t}   & \sim \mbox{N}(\mu_{g}, \sigma^{2}_{g}), \\
e_{j}|\mathcal{T}_{t}   & \sim \mbox{N}(\mu_{e}, \sigma^{2}_{e}), \\
\sigma^{2}_{g} & \sim \mbox{IG}(a_{g}, b_{g}), \\
\sigma^{2}_{e} & \sim \mbox{IG}(a_{e}, b_{e}), \\
\sigma^{2} & \sim \mbox{IG}(a, b), \\
\end{aligned}
$$

Similarly to the original BART, the conditional posterior distribution of $\mu_{t \ell}$ in the AMBARTI also depends only on the information provided by all the other trees ($T_{(t)}$) and means ($M_{(t)}$) via partial residual in the form of
$$
\begin{aligned}
r_{ij} & = y_{ij} - \left( g_{i} + e_{j} + \sum_{t=1}^{T} h(\textbf{x}_{ij}, \mathcal{M}_{t}, \mathcal{T}_{t}) \right)\\
r_{ij} & \stackrel{\mathclap{\tiny \mbox{iid}}}{\sim} \mbox{N} \left( g_{i} + e_{j} + \sum_{t=1}^{T} h(\textbf{x}_{ij}, \mathcal{M}_{t}, \mathcal{T}_{t}), \sigma^{2} \right).
\end{aligned}
$$
Below, we list all full conditionals needed.


The joint posterior distribution of the trees and predicted values is given by
$$
\begin{aligned}
p(\mathcal{T},\mathcal{M}, \mathcal{G}, \mathcal{E}, \sigma^{2}_{g}, \sigma^{2}_{e}, \sigma^{2}| \textbf{y}, \textbf{X}) & \propto p(\textbf{y}| \textbf{X}, \mathcal{T}, \mathcal{M}, \mathcal{G}, \mathcal{E}, \sigma^{2}_{g}, \sigma^{2}_{e}, \sigma^{2}) p(\mathcal{G}) p(\mathcal{E}) p(\mathcal{M}|\mathcal{T}) p(\mathcal{T}) p(\sigma^{2}_{g}) p(\sigma^{2}_{e}) p(\sigma^{2}), \\
p(\mathcal{T},\mathcal{M}, \Theta, \sigma^{2}|\textbf{y}, \textbf{X})
& \propto \left[\prod_{t=1}^{m}\prod_{\ell = 1}^{b_{t}} \prod_{i: \textbf{x}_{ij} \in \mathcal{P}_{t\ell}} \prod_{j: \textbf{x}_{ij} \in \mathcal{P}_{t\ell}} p(y_{ij}| \textbf{x}_{ij}, \mathcal{T}_{t}, \mathcal{M}_{t}, \Theta, \sigma^{2}) \right] \left[ \prod_{i} p(g_{i}) \right] \left[ \prod_{j} p(e_{j}) \right] \times \\
& \mbox{ } \mbox{ } \mbox{ } \mbox{ }  \times \left[\prod_{t=1}^{m} \prod_{\ell = 1}^{b_{t}} p(\mu_{t \ell}|\mathcal{T}_{t}) p(\mathcal{T}_{t}) \right] p(\sigma^{2}_{g}) p(\sigma^{2}_{e}) p(\sigma^{2}).
\end{aligned}
$$
Let $R_{t}$ denote the vector of the partial residuals, and $\textbf{g} \in \mathbb{R}^{n}$ and $\textbf{e} \in \mathbb{R}^{n}$ are vectors containing the random effects $g_{i}$ and $e_{j}$ for all observations.
$$
\begin{aligned}
R_{t} & = \textbf{y} - \left( \textbf{g} + \textbf{e} +\sum_{k \neq t}^{T} h(\textbf{X}; \mathcal{T}_{k}, \mathcal{M}_{k}) \right), \\
r_{ij}^{(t)} & \stackrel{\mathclap{\tiny \mbox{iid}}}{\sim} \mbox{N} \left( g_{i} + e_{j} + h(\textbf{x}_{ij}; \mathcal{T}_{t}, \mathcal{M}_{t}), \sigma^{2} \right).
\end{aligned}
$$
As all $\mu_{t \ell}$ are independent from each other, it is possible to write $p(\mathcal{M}_{t}| \mathcal{T}_{t}, R_{t}, \sigma^{2}) = \prod_{\ell = 1}^{b_{t}} p(\mu_{t \ell}|\mathcal{T}_{t}, R_{t}, \sigma^{2})$. Hence,
$$
\begin{aligned}
p(\mu_{t \ell}| \mathcal{T}_{t}, R_{t}, \sigma^{2}) & \propto p(R_{t}| \mathcal{M}_{t}, \mathcal{T}_{t}, \sigma^{2}) p(\mu_{t \ell}), \\
& \propto \exp \left( - \frac{1}{2 \sigma^{2}_{*}} \left( \mu_{t \ell} - \mu_{t \ell}^{*} \right)^{2}  \right),
\end{aligned}
$$
which is a 

\begin{align}
\mbox{N}\left(\frac{ \sigma^{-2}\sum_{(i,j) \in \mathcal{P}_{t \ell}} r_{ij}^{(t)}}{n_{t \ell}/\sigma^{2} + \sigma^{-2}_{\mu}}, \frac{1}{n_{t \ell}/\sigma^{2} + \sigma^{-2}_{\mu}} \right),
\end{align}

where $r_{ij}^{(t)} = y_{ij} - (g_{i} + e_{j} + \sum_{k \neq t}^{T} h(\textbf{x}_{ij}; \mathcal{T}_{k}, \mathcal{M}_{k}))$.

$$
\begin{aligned}
p(g_{i}| \textbf{y}, \sigma^{2}_{g}, \sigma^{2}) & \propto p(\textbf{y}| g_{i}, \sigma^{2}_{g}, \sigma^{2}) p(g_{i}), \\
& \propto \exp \left( - \frac{1}{2 \sigma^{2}_{g*}} \left( g_{i} - g_{i}^{*} \right)^{2}  \right),
\end{aligned}
$$
which is a 

\begin{align}
\mbox{N}\left(\frac{\sum_{j} \left[y_{ij} - e_{j} - \hat{\mu}_{ij}\right]/\sigma^{2} + \mu_{g}/\sigma^{2}_{g}}{n_{g_{i}}/ \sigma^{2} + 1/\sigma^{2}_{g}}, \frac{1}{n_{g_{i}}/ \sigma^{2} + 1/\sigma^{2}_{g}} \right),
\end{align}

where $\hat{\mu}_{ij} = \sum_{t=1}^{T} h(\textbf{x}_{ij}, \mathcal{T}_{t}, \mathcal{M}_{t})$, $r_{ij} \in R$, and $n_{g_{i}}$ is the number of observations that belong to $g_{i}$.
$$
\begin{aligned}
p(e_{j}| \textbf{y}, \sigma^{2}_{e}, \sigma^{2}) & \propto p(\textbf{y}| e_{j}, \sigma^{2}_{e}, \sigma^{2}) p(e_{j}), \\
& \propto \exp \left( - \frac{1}{2 \sigma^{2}_{e*}} \left( e_{j} - e_{j}^{*} \right)^{2}  \right),
\end{aligned}
$$
which is a 

\begin{align}
\mbox{N}\left(\frac{\sum_{i} \left[y_{ij} - g_{i} - \hat{\mu}_{ij}\right]/\sigma^{2} + \mu_{e}/\sigma^{2}_{e}}{n_{e_{j}}/ \sigma^{2} + 1/\sigma^{2}_{e}}, \frac{1}{n_{e_{j}}/ \sigma^{2} + 1/\sigma^{2}_{e}} \right).
\end{align}

$$
\begin{aligned}
p(\sigma^{2}_{g} | \mathcal{G}) \propto & \mbox{ } p(\mathcal{G} | \sigma^{2}_{g}) p(\sigma^{2}_{g}) \nonumber\\
\propto & \mbox{ } (\sigma^{2}_{g})^{-\left(\frac{XX + a_{g}}{2} + 1\right)} \exp \left( -\frac{YY + 2 b_{g}}{2} \right),
\end{aligned}
$$
which is an 
\begin{align}
\mbox{IG}\left(\frac{|\mathcal{G}|}{2} + a_{g}, \frac{\sum_{i=1}^{I} (g_{i}-\mu_{g})^{2}}{2} + b_{g} \right).
\end{align}
$$
\begin{aligned}
p(\sigma^{2}_{e} | \mathcal{G}) \propto & \mbox{ } p(\mathcal{E} | \sigma^{2}_{e}) p(\sigma^{2}_{e}) \nonumber\\
\propto & \mbox{ } (\sigma^{2}_{e})^{-\left(\frac{XX + a_{e}}{2} + 1\right)} \exp \left( -\frac{YY + 2 b_{e}}{2} \right),
\end{aligned}
$$
which is an
\begin{align}
\mbox{IG}\left(\frac{|\mathcal{E}|}{2} + a_{e}, \frac{\sum_{j=1}^{E} (e_{j}-\mu_{e})^{2}}{2} + b_{e} \right).
\end{align}
Then, after generating all predicted values for all trees, $\sigma^{2}$ can be updated based on
\begin{align}
\label{BART_full_conditional_sigma2}
p(\sigma^{2} | T, M, \textbf{X}, \textbf{y}) \propto & \mbox{ } p(\textbf{y} | \textbf{X}, T, M, \sigma^{2}) p(\sigma^{2}) \nonumber\\
\propto & \mbox{ } (\sigma^{2})^{-\left(\frac{n + \nu}{2} + 1\right)} \exp \left( -\frac{S + \nu \lambda}{2\sigma^{2}} \right),
\end{align}
where $S = \sum_{i=1}^{n} (y_{i} - \hat{y}_{i})^{2}$ and $\hat{y}_{i} = \textbf{g} + \textbf{e} + \sum_{t = 1}^{T} h(\textbf{x}_{ij}; \mathcal{T}_{t}, \mathcal{M}_{t})$. The expression in \eqref{BART_full_conditional_sigma2} is an $\mbox{IG}((n + \nu)/2, (S + \nu \lambda)/2)$, and drawing samples from it is straightforward.

$$
\begin{aligned}
p(\mathcal{T}_{t}| R_{t}, \sigma^{2}) &  \propto p(\mathcal{T}_{t}) \int p(R_{t}| \mathcal{M}_{t}, \mathcal{T}_{t}, \sigma^{2}) p(\mathcal{M}_{t} | \mathcal{T}_{t}) d\mathcal{M}_{t}, \\
& \propto p(\mathcal{T}_{t}) p(R_{t}| \mathcal{T}_{t}, \sigma^{2}), \\
& \propto p(\mathcal{T}_{t}) \prod_{\ell = 1}^{b_{t}} \left[ \left(\frac{\sigma^{2}}{\sigma^{2}_{\mu} n_{t \ell} + \sigma^{2}}\right)^{1/2} \exp \left( \frac{\sigma^{2}_{\mu} \left[ n_{t \ell} \bar{R}_{\ell} \right]^2}{2 \sigma^{2} (\sigma^{2}_{\mu} n_{t \ell} + \sigma^{2})} \right) \right],
\end{aligned}
$$
where $\bar{R}_{\ell} = \sum_{(i,j) \in \mathcal{P}_{t \ell}} (r_{ij}^{(t)} - g_{i} - e_{j}) / n_{t \ell}$, $r_{ij}^{(t)} \in R_{t}$ and $n_{t \ell}$ is the number of observations that belong to $\mathcal{P}_{t \ell}$. This sampling is carried out through a Metropolis-Hastings step, as the expression does not have a known distributional form;

## Orthonormality constraints

Below, we show how to apply the orthonormality constraints on $\gamma_{iq}$ and $\delta_{jq}$ when simulating from the AMMI model. To illustrate the strategy, we consider that $\boldsymbol\gamma \in \mathbb{R}^{I\times Q}$ and $S \in \mathbb{R}^{I\times Q}$, where $s_{ij} \sim \mbox{N}(0, \sigma^2_{x})$ - actually, $s_{ij}$ could be sampled from many other distributions, such as Gamma or Beta. Also, define $M$ as an $I \times Q$ matrix with each element being the mean of the corresponding $Q$ columns of $S$. 

Recall that the orthonormality constraints impose that $\textbf{1}_{I}^{\top}\boldsymbol\gamma_{q} = 0$ and $\boldsymbol\gamma^\top\boldsymbol\gamma = \mathbb{I}$, where $\textbf{1}_{I}$ is a row vector of dimension $I$ and $\boldsymbol\gamma_{q}$ is a column of the matrix $\boldsymbol\gamma$. Initially, we know that

$$
\begin{aligned}
B & = S - M \\
& \Rightarrow \textbf{1}_{I}^{\top} B = 0 \\
& \nRightarrow B^{\top} B = \mathbb{I}, \\
& \end{aligned}
$$
where $B$ is, by construction, a full rank matrix and $B^\top B$ is symmetric. In this sense, we need to find a matrix, let's say, $A$, such that $C = BA \Rightarrow C^\top C = \mathbb{I}$. To do so, we know that
$$
\begin{aligned}
D & = C^\top C = \mathbb{I}\\
 & \Rightarrow (BA)^\top BA = \mathbb{I}\\
 & \Rightarrow A^\top B^\top BA = \mathbb{I} \\
 & \Rightarrow A^\top B^\top B = A^{-1} \\
 & \Rightarrow  B^\top B = A^{-\top}A^{-1} \\
 & \Rightarrow  B^\top B = (A A^{\top})^{-1} \\
 & \Rightarrow  (B^\top B)^{-1} = A A^{\top} \\
 & \Rightarrow  (B^\top B)^{-1} = A^{2} \mbox{ (by symmetry)} \\
 & \Rightarrow  (B^\top B)^{-1/2} = A.
\end{aligned}
$$